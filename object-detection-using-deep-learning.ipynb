{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"464e14da550dff992448d94ebb1b764bdfc75fa220589997441054f618fc8a40"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/babe8901/object-detection-using-deep-learning?scriptVersionId=107759138\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"! pip install numpy\n! pip install pandas\n! pip install matplotlib\n! pip install opencv-python\n! pip install tqdm\n! pip install sklearn\n! pip install pyyaml\n! pip install torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport cv2 \nimport os \nimport shutil \nfrom tqdm.notebook import tqdm \nfrom sklearn.model_selection import train_test_split\nimport yaml\nimport torch","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations = pd.read_csv('../input/car-object-detection/data/train_solution_bounding_boxes (1).csv')\nannotations.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Looking at some examples from training set","metadata":{}},{"cell_type":"code","source":"base_path = '../input/car-object-detection/data/training_images'\nfig, ax = plt.subplots(figsize=(16,16), nrows=4, ncols=4)\nnrows=4\nncols=4\nindices = np.random.randint(low=0, high=len(annotations), size=(nrows, ncols))\nfor i in range(nrows):\n    for j in range(ncols):\n        idx = indices[i,j]\n        row = annotations.loc[idx]\n        start = (int(row['xmin']), int(row['ymin']))\n        end = (int(row['xmax']), int(row['ymax']))\n        img_path = os.path.join(base_path, row['image'])\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.rectangle(img, start, end, color=(0,255,0), thickness=2)\n        ax[i,j].axis('off')\n        ax[i,j].imshow(img)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making annotations compatible with YOLO","metadata":{}},{"cell_type":"code","source":"annotations['xcenter'] = (annotations['xmin']+annotations['xmax'])/2\nannotations['ycenter'] = (annotations['ymin']+annotations['ymax'])/2\nannotations['width'] = (annotations['xmax'] - annotations['xmin'])\nannotations['height'] = (annotations['ymax'] - annotations['ymin'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h,w,c = img.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations['xcenter'] = annotations['xcenter']/w\nannotations['width'] = annotations['width']/w\nannotations['ycenter'] = annotations['ycenter']/h\nannotations['height'] = annotations['height']/h\nannotations['image'] = annotations['image'].apply(lambda x: os.path.join(base_path,x))\nannotations = annotations[['image', 'xcenter','ycenter','width','height']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating train, test and val folders","metadata":{}},{"cell_type":"code","source":"train_path = os.path.join('Data','train')\ntest_path = os.path.join('Data','test')\nval_path = os.path.join('Data','val')\n\nos.makedirs(train_path, exist_ok=True)\nprint('Made folder for train images')\nos.makedirs(val_path, exist_ok=True)\nprint('Made folder for validation images')\nos.makedirs(test_path, exist_ok=True)\nprint('Made folder for test images')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_size = int(0.1 * len(annotations))\ntrain_df, test_df = train_test_split(annotations, test_size=test_size)\ntrain_df, val_df = train_test_split(train_df, test_size=test_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = {\n    'train':{\n        'df':train_df,\n        'path':train_path\n    },\n    'val':{\n        'df':val_df,\n        'path':val_path\n    },\n    'test':{\n        'df':test_df,\n        'path':test_path\n    }\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Moving the files to train, val and test folders","metadata":{}},{"cell_type":"code","source":"for subset, subset_data in data.items():\n    df = subset_data['df']\n    dst_path = subset_data['path']\n    for idx, row in tqdm(df.iterrows()):\n        src = row['image']\n        img_id = src.split(os.path.sep)[-1].split('.')[0]\n        extension = src.split(os.path.sep)[-1].split('.')[1]\n        img_dst = os.path.join(dst_path, f'{img_id}.{extension}')\n        shutil.copy2(src, img_dst)\n        annotation_text = f\"0 {row['xcenter']} {row['ycenter']} {row['width']} {row['height']}\"\n        with open(os.path.join(dst_path, f'{img_id}.txt'), 'w+') as f:\n            f.write(annotation_text)\n            f.write('\\n')\n    print(f'Done moving files for {subset} set')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {\n    'names':['Car'],\n    'nc':1,\n    'train':os.path.abspath(train_path),\n    'val':os.path.abspath(val_path),\n    'test':os.path.abspath(test_path)\n}\n\nwith open('data.yaml', 'w+') as f:\n    yaml.dump(config, f, default_flow_style=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! git clone https://github.com/ultralytics/yolov5.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install -r yolov5/requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! wandb disabled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! python yolov5/train.py --data data.yaml --img 640 --batch 16 --epochs 20 --weights yolov5/yolov5s.pt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5/runs/train/exp/weights/best.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference on test images","metadata":{}},{"cell_type":"code","source":"test_base_path = '../input/car-object-detection/data/testing_images'\ntest_img = os.listdir(test_base_path)\ntest_img = list(map(lambda x: os.path.join(test_base_path, x), test_img))\ntest_images = np.random.choice(test_img, size=(4,4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16,16), nrows=4, ncols=4)\nfor i in range(4):\n    for j in range(4):\n        image = cv2.imread(test_images[i,j])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        results = best_model(image)\n        ax[i,j].axis('off')\n        ax[i,j].imshow(np.squeeze(results.render()))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}